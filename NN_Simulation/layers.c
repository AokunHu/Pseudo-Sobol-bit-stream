// This file mainly contains some activation functions and function that implements fully connection
#include<stdio.h>
#include<stdlib.h>
#include<math.h>
#include<malloc.h>
#include"layers.h"
#include <string.h> //add by aokun
#include"quan.h"

int get_first_one(const double decp,const int maxp)
{
  double xs=decp;  // 小数部分。
  int qz=0;        // 取整的结果。
  int ii=0;        // 位数的计数器。

  while (xs>=0)
  {
    qz=(int)(xs*2);      // 小数部分乘2取整。
	if (qz==1) return ii;
	ii=ii+1;
    xs=xs*2;             // 小数部分乘2。
    if (xs>=1) xs=xs-1;  // 如果>=1，取整。
    
    if (ii==maxp) return ii;
  }

}

int sc_single_bit_mul(int first_one_a, int first_one_b, double a, double b)
{
  double xs;  // 小数部分。
  int qz=0;        // 取整的结果。
  int ii=0;        // 位数的计数器。
  int a_result;
  int b_result;
  
  xs=a;
  for(ii=0;ii<=first_one_a;ii++)
  {
	qz=(int)(xs*2); 
	xs=xs*2;
    if (xs>=1) xs=xs-1;
  }
  a_result=qz;

  xs=b;
  for(ii=0;ii<=first_one_b;ii++)
  {
	qz=(int)(xs*2); 
	xs=xs*2;
    if (xs>=1) xs=xs-1;
  }
  b_result=qz;
  //if(a*b!=0) printf("a_result=%d,b_result=%d\n", a_result, b_result);
  if(a_result==1 && b_result==1)
  {
	return 1;
  }
  else
  {
    return 0;
  }

}

int sc_single_bit_mul_fast(int first_one_a, int first_one_b, int *binary_a, int *binary_b)
{
  //if(a*b!=0) printf("a_result=%d,b_result=%d\n", a_result, b_result);
  if(binary_a[first_one_a]==1 && binary_b[first_one_b]==1)
  {
	return 1;
  }
  else
  {
    return 0;
  }

}

void dec2bin(double a, int len, int*binary)
{
  double xs;
  double qz;
  int ii;
  xs=a;
  for(ii=0;ii<len;ii++)
  {
	qz=(int)(xs*2); 
	if (qz==1) binary[ii]=1;
	else binary[ii]=0;
	xs=xs*2;
    if (xs>=1) xs=xs-1;
  }
  return;
}





double SC_Mul
(
	double a, // Kernel size is m*n
	double b,
	int len,
	int *first_one_a,
	int *first_one_b //Sobol number[3][1023]
)
{
	int i;
	int bitwidth;

	double result=0;
	double abs_a;
	double abs_b;
	int *abs_a_binary;
	int *abs_b_binary;
	bitwidth=log(len)/log(2);
	abs_a = fabs(a);
	abs_b = fabs(b);
	abs_a_binary=(int*)malloc(sizeof(int)*bitwidth);
	abs_b_binary=(int*)malloc(sizeof(int)*bitwidth);
	dec2bin(abs_a,bitwidth,abs_a_binary);
	dec2bin(abs_b,bitwidth,abs_b_binary);
	// //debug
	// printf("a=%f,b=%f\n",i, a,b);
	// for(i=0;i<bitwidth;i++)
	// {
	// 	if(a*b!=0) printf("abs_a_binary[%d]=%d,abs_b_binary[%d]=%d\n",i, abs_a_binary[i],i, abs_b_binary[i]);
	// }
	// //enddebug
	if (a==1 && b!=0) return b;
	if (b==1 && a!=0) return a;
    for(i=0;i<len;i++)
	{
		// if(first_one_a!=bitwidth && first_one_b!=bitwidth) result+=sc_single_bit_mul(first_one_a,first_one_b,abs_a,abs_b);
		if(first_one_a[i]!=bitwidth && first_one_b[i]!=bitwidth) result+=sc_single_bit_mul_fast(first_one_a[i],first_one_b[i],abs_a_binary,abs_b_binary);
		// if(a*b!=0) printf("%d: first_one_a=%d,first_one_b=%d,result=%f\n",i, first_one_a[i], first_one_b[i],result);
	}
	if (a*b>=0) result=result/len;
	else result=-result/len;
        
	free(abs_a_binary);
	free(abs_b_binary);
	return result;
}


double SC_Mul_Comp //this function is used to do the multiplication of SC. SC is generated by compared with sobol
(
	double a, // Kernel size is m*n
	double b,
	int len,
	double **Sobol_seq //Sobol number[3][1023]
)
{
	int i;
	int bitwidth;
	int first_one_a;
	int first_one_b;
	double result=0;
	double abs_a;
	double abs_b;
	// int rand_a;
	// int rand_b;
	// double rand_a_decimal;
	// double rand_b_decimal;
	bitwidth=log(len)/log(2);
	abs_a = fabs(a);
	abs_b = fabs(b);
    for(i=0;i<len;i++)
	{
        // rand_a=rand()%256;
		// rand_b=rand()%256;
		// rand_a_decimal=rand_a/256.0;
		// rand_b_decimal=rand_b/256.0;
		if(abs_a>=Sobol_seq[0][i] && abs_b>=Sobol_seq[1][i]) result+=1;
		// if(abs_a>=rand_a_decimal && abs_b>=rand_b_decimal) result+=1;
		//if(a*b!=0) printf("%d: first_one_a=%d,first_one_b=%d,a=%f,b=%f,result=%f\n",i, first_one_a, first_one_b,abs_a,abs_b,result);
	}
	if (a*b>=0) result=result/len;
	else result=-result/len;
        
	return result;
}



int free_split_param // This is the function used to generate relative parameters for splitting weights according to their signs (positive or negative)
(
	int num_layer,// Number of layers, excluding the input layer
	int *num_neuron, // Numbers of neurons for different layers, excluding the input layer
	double ***weights_pos, // Positive weights for different layers
	double ***weights_neg, // Negative weights for different layers
	int **weights_pos_num, // Number of positive weights for different layers
	int **weights_neg_num, // Number of negative weights for different layers
	int ***weights_pos_idx, // The indices that positive weights hold in the original order
	int ***weights_neg_idx // The indices that negative weights hold in the original order
)
{
	int layer,i,j;
	for(layer=0;layer<num_layer;layer++)
	{
		
		for(i=0;i<num_neuron[layer];i++)
		{
			free(weights_neg[layer][i]);
			free(weights_pos[layer][i]);
			free(weights_neg_idx[layer][i]);
			free(weights_pos_idx[layer][i]);
		}
		free(weights_pos[layer]);
		free(weights_neg[layer]);
		free(weights_pos_idx[layer]);
		free(weights_neg_idx[layer]);
		free(weights_pos_num[layer]);
		free(weights_neg_num[layer]);
	}
	free(weights_pos);
	free(weights_neg);
	free(weights_pos_idx);
	free(weights_neg_idx);
	free(weights_pos_num);
	free(weights_neg_num);
	
	return 0;
}



int FireChen
(
	int num_in_channel,
	int num_out_channel,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map
	int W_out, // Width of output feature map
	int s11, // Number of 1*1 squeeze filters
	int e11, // Number of 1*1 expand filters
	int e33, // Number of 3*3 expand filters
	double *****weight, // Weights of the three kinds of filters
	double **bias, // Biases of the three kinds of filters
	double ***input, // Input feature map
	double ***output // Output feature map
)
{
	int i,j,k,ii,jj,kk;
	
	
	// Squeeze convolutional kernals, which have been combined with BN
	double ***output_s11;
	output_s11=(double ***)malloc(sizeof(double **)*s11);
	for(i=0;i<s11;i++)
	{
		output_s11[i]=(double **)malloc(sizeof(double *)*H_in);
		for(j=0;j<H_in;j++)
			output_s11[i][j]=(double *)malloc(sizeof(double )*W_in);
	}
	
	Conv // Convolutional layer
	(
	1, // Kernel size is m*n
	1,
	H_in, // Height of input feature map
	H_out, // Width of input feature map
	H_in, // Height of output feature map
	H_out, // Width of output feature map
	num_in_channel, // Number of input channels
	s11, // Number of output channels
	weight[0], // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	bias[0], // num_out_channel biases
	1,
	input, // num_in_channel*H*W
	output_s11 // num_out_channel*H_out*W_out
	);
	/*
	if(num_in_channel==96)
	{
		printf("input:\n");
		j=0;
		k=4;
		for(i=0;i<96;i++)
			printf("%lf ",input[i][j][k]);
		printf("\n");
		printf("weight:\n");
		i=0;
		for(j=0;j<96;j++)
			printf("%lf ",weight[0][i][j][0][0]);
		printf("\n");
	}
	*/
	for(i=0;i<s11;i++)
	{
		for(j=0;j<H_in;j++)
		{
			for(k=0;k<W_in;k++)
			{
				//if(num_in_channel==96 && i==0 && j==0)printf("output_s11[%d][%d][%d]=%lf\n",i,j,k,output_s11[i][j][k]);
				if(output_s11[i][j][k]<0)
					output_s11[i][j][k]=0;
			}
		}
	}
	
	
	
	// Expand 1*1 convolutional kernals, which have been combined with BN
	double ***output_e11;
	output_e11=(double ***)malloc(sizeof(double **)*e11);
	for(i=0;i<e11;i++)
	{
		output_e11[i]=(double **)malloc(sizeof(double *)*H_in);
		for(j=0;j<H_in;j++)
			output_e11[i][j]=(double *)malloc(sizeof(double )*W_in);
	}
	
	Conv // Convolutional layer
	(
	1, // Kernel size is m*n
	1,
	H_in, // Height of input feature map
	H_out, // Width of input feature map
	H_in, // Height of output feature map
	H_out, // Width of output feature map
	s11, // Number of input channels
	e11, // Number of output channels
	weight[1], // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	bias[1], // num_out_channel biases
	1,
	output_s11, // num_in_channel*H*W
	output_e11 // num_out_channel*H_out*W_out
	);
	
	for(i=0;i<e11;i++)
	{
		for(j=0;j<H_in;j++)
		{
			for(k=0;k<W_in;k++)
			{
				if(output_e11[i][j][k]<0)
					output_e11[i][j][k]=0;
			}
		}
	}
	
	
	
	//Padding
	double ***output_s11_pad; // The input for 3*3 expand convolutional kernals is padded
	output_s11_pad=(double ***)malloc(sizeof(double **)*s11);
	for(i=0;i<s11;i++)
	{
		output_s11_pad[i]=(double **)malloc(sizeof(double *)*(H_in+2));
		for(j=0;j<H_in+2;j++)
		{
			output_s11_pad[i][j]=(double *)malloc(sizeof(double )*(W_in+2));
			if(j==0 || j==H_in+1)
			{
				for(k=0;k<W_in+2;k++)
					output_s11_pad[i][j][k]=0;
			}
			else
			{
				for(k=0;k<W_in+2;k++)
				{
					if(k==0 || k==W_in+1)
						output_s11_pad[i][j][k]=0;
					else
						output_s11_pad[i][j][k]=output_s11[i][j-1][k-1];
				}
			}
		}
	}
	
	
	// Expand 3*3 convolutional kernals, which have been combined with BN
	double ***output_e33;
	output_e33=(double ***)malloc(sizeof(double **)*e33);
	for(i=0;i<e33;i++)
	{
		output_e33[i]=(double **)malloc(sizeof(double *)*H_in);
		for(j=0;j<H_in;j++)
			output_e33[i][j]=(double *)malloc(sizeof(double )*W_in);
	}
	
	Conv // Convolutional layer
	(
	3, // Kernel size is m*n
	3,
	H_in+2, // Height of input feature map
	H_out+2, // Width of input feature map
	H_in, // Height of output feature map
	H_out, // Width of output feature map
	s11, // Number of input channels
	e33, // Number of output channels
	weight[2], // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	bias[2], // num_out_channel biases
	1,
	output_s11_pad, // num_in_channel*H*W
	output_e33 // num_out_channel*H_out*W_out
	);
	
	for(i=0;i<e33;i++)
	{
		for(j=0;j<H_in;j++)
		{
			for(k=0;k<W_in;k++)
			{
				if(output_e33[i][j][k]<0)
					output_e33[i][j][k]=0;
			}
		}
	}
	
	
	//Concatenate
	for(i=0;i<num_out_channel;i++)
	{
		if(i<e11)
		{
			for(j=0;j<H_in;j++)
				for(k=0;k<W_in;k++)
					output[i][j][k]=output_e11[i][j][k];
		}
		else
		{
			for(j=0;j<H_in;j++)
				for(k=0;k<W_in;k++)
					output[i][j][k]=output_e33[i-e11][j][k];
		}
	}
	
	for(i=0;i<s11;i++)
	{
		for(j=0;j<H_in;j++)
			free(output_s11[i][j]);
		free(output_s11[i]);
	}
	free(output_s11);
	
	for(i=0;i<e11;i++)
	{
		for(j=0;j<H_in;j++)
			free(output_e11[i][j]);
		free(output_e11[i]);
	}
	free(output_e11);
	
	for(i=0;i<e33;i++)
	{
		for(j=0;j<H_in;j++)
			free(output_e33[i][j]);
		free(output_e33[i]);
	}
	free(output_e33);
	
	for(i=0;i<s11;i++)
	{
		for(j=0;j<H_in+2;j++)
			free(output_s11_pad[i][j]);
		free(output_s11_pad[i]);
	}
	free(output_s11_pad);
	
	return 0;
}




int FireChen_SC
(
	int num_in_channel,
	int num_out_channel,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map
	int W_out, // Width of output feature map
	int s11, // Number of 1*1 squeeze filters
	int e11, // Number of 1*1 expand filters
	int e33, // Number of 3*3 expand filters
	double *****weight, // Weights of the three kinds of filters
	double **bias, // Biases of the three kinds of filters
	double ***input, // Input feature map
	double ***output, // Output feature map
	double **Sobol_seq, //Sobol number[3][1023]
	int len
)
{
	int i,j,k,ii,jj,kk;
	
	
	// Squeeze convolutional kernals, which have been combined with BN
	double ***output_s11;
	output_s11=(double ***)malloc(sizeof(double **)*s11);
	for(i=0;i<s11;i++)
	{
		output_s11[i]=(double **)malloc(sizeof(double *)*H_in);
		for(j=0;j<H_in;j++)
			output_s11[i][j]=(double *)malloc(sizeof(double )*W_in);
	}
	
	Conv_SC_Sobol // Convolutional layer
	(
	1, // Kernel size is m*n
	1,
	H_in, // Height of input feature map
	H_out, // Width of input feature map
	H_in, // Height of output feature map
	H_out, // Width of output feature map
	num_in_channel, // Number of input channels
	s11, // Number of output channels
	weight[0], // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	bias[0], // num_out_channel biases
	1,
	input, // num_in_channel*H*W
	output_s11, // num_out_channel*H_out*W_out
	Sobol_seq,
	len
	);
	/*
	if(num_in_channel==96)
	{
		printf("input:\n");
		j=0;
		k=4;
		for(i=0;i<96;i++)
			printf("%lf ",input[i][j][k]);
		printf("\n");
		printf("weight:\n");
		i=0;
		for(j=0;j<96;j++)
			printf("%lf ",weight[0][i][j][0][0]);
		printf("\n");
	}
	*/
	for(i=0;i<s11;i++)
	{
		for(j=0;j<H_in;j++)
		{
			for(k=0;k<W_in;k++)
			{
				//if(num_in_channel==96 && i==0 && j==0)printf("output_s11[%d][%d][%d]=%lf\n",i,j,k,output_s11[i][j][k]);
				if(output_s11[i][j][k]<0)
					output_s11[i][j][k]=0;
			}
		}
	}
	
	
	
	// Expand 1*1 convolutional kernals, which have been combined with BN
	double ***output_e11;
	output_e11=(double ***)malloc(sizeof(double **)*e11);
	for(i=0;i<e11;i++)
	{
		output_e11[i]=(double **)malloc(sizeof(double *)*H_in);
		for(j=0;j<H_in;j++)
			output_e11[i][j]=(double *)malloc(sizeof(double )*W_in);
	}
	
	Conv_SC_Sobol // Convolutional layer
	(
	1, // Kernel size is m*n
	1,
	H_in, // Height of input feature map
	H_out, // Width of input feature map
	H_in, // Height of output feature map
	H_out, // Width of output feature map
	s11, // Number of input channels
	e11, // Number of output channels
	weight[1], // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	bias[1], // num_out_channel biases
	1,
	output_s11, // num_in_channel*H*W
	output_e11, // num_out_channel*H_out*W_out
	Sobol_seq,
	len
	);
	
	for(i=0;i<e11;i++)
	{
		for(j=0;j<H_in;j++)
		{
			for(k=0;k<W_in;k++)
			{
				if(output_e11[i][j][k]<0)
					output_e11[i][j][k]=0;
			}
		}
	}
	
	
	
	//Padding
	double ***output_s11_pad; // The input for 3*3 expand convolutional kernals is padded
	output_s11_pad=(double ***)malloc(sizeof(double **)*s11);
	for(i=0;i<s11;i++)
	{
		output_s11_pad[i]=(double **)malloc(sizeof(double *)*(H_in+2));
		for(j=0;j<H_in+2;j++)
		{
			output_s11_pad[i][j]=(double *)malloc(sizeof(double )*(W_in+2));
			if(j==0 || j==H_in+1)
			{
				for(k=0;k<W_in+2;k++)
					output_s11_pad[i][j][k]=0;
			}
			else
			{
				for(k=0;k<W_in+2;k++)
				{
					if(k==0 || k==W_in+1)
						output_s11_pad[i][j][k]=0;
					else
						output_s11_pad[i][j][k]=output_s11[i][j-1][k-1];
				}
			}
		}
	}
	
	
	// Expand 3*3 convolutional kernals, which have been combined with BN
	double ***output_e33;
	output_e33=(double ***)malloc(sizeof(double **)*e33);
	for(i=0;i<e33;i++)
	{
		output_e33[i]=(double **)malloc(sizeof(double *)*H_in);
		for(j=0;j<H_in;j++)
			output_e33[i][j]=(double *)malloc(sizeof(double )*W_in);
	}
	
	Conv_SC_Sobol // Convolutional layer
	(
	3, // Kernel size is m*n
	3,
	H_in+2, // Height of input feature map
	H_out+2, // Width of input feature map
	H_in, // Height of output feature map
	H_out, // Width of output feature map
	s11, // Number of input channels
	e33, // Number of output channels
	weight[2], // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	bias[2], // num_out_channel biases
	1,
	output_s11_pad, // num_in_channel*H*W
	output_e33, // num_out_channel*H_out*W_out
	Sobol_seq,
	len
	);
	
	for(i=0;i<e33;i++)
	{
		for(j=0;j<H_in;j++)
		{
			for(k=0;k<W_in;k++)
			{
				if(output_e33[i][j][k]<0)
					output_e33[i][j][k]=0;
			}
		}
	}
	
	
	//Concatenate
	for(i=0;i<num_out_channel;i++)
	{
		if(i<e11)
		{
			for(j=0;j<H_in;j++)
				for(k=0;k<W_in;k++)
					output[i][j][k]=output_e11[i][j][k];
		}
		else
		{
			for(j=0;j<H_in;j++)
				for(k=0;k<W_in;k++)
					output[i][j][k]=output_e33[i-e11][j][k];
		}
	}
	
	for(i=0;i<s11;i++)
	{
		for(j=0;j<H_in;j++)
			free(output_s11[i][j]);
		free(output_s11[i]);
	}
	free(output_s11);
	
	for(i=0;i<e11;i++)
	{
		for(j=0;j<H_in;j++)
			free(output_e11[i][j]);
		free(output_e11[i]);
	}
	free(output_e11);
	
	for(i=0;i<e33;i++)
	{
		for(j=0;j<H_in;j++)
			free(output_e33[i][j]);
		free(output_e33[i]);
	}
	free(output_e33);
	
	for(i=0;i<s11;i++)
	{
		for(j=0;j<H_in+2;j++)
			free(output_s11_pad[i][j]);
		free(output_s11_pad[i]);
	}
	free(output_s11_pad);
	
	return 0;
}




int FC // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output
)
{
	int i,j;
	//printf("weights[0][0]=%lf\n",weights[0][0]);
	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			output[i] += weights[i][j] * input[j];
			//if(in_neuron==480 && i==0)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
		}
	}
	
	return 0;
}


int FC_quan // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output,
	int integer, 
	double decimal, 
	int decimal_shift,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2
)
{
	int i,j;
	
	for(i = 0; i < out_neuron; i++)
	{
		bias[i]=quan_i_d(bias[i], integer,decimal, decimal_shift);
		for (j = 0; j < in_neuron; j++)
		{
			weights[i][j]=quan_i_d(weights[i][j], integer,decimal, decimal_shift);
		}
	}
	
	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			output[i] += weights[i][j] * input[j];
			output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
			//if(in_neuron==480 && i==0)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
		}
	}
	
	return 0;
}



int FC_quan_sc_sobol // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output,
	int integer, 
	double decimal, 
	int decimal_shift,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2,
	double **Sobol_seq, //Sobol number[3][1023]
	int len
)
{
	int i,j;
	double part_result;
    
	int *first_one_a;
	int *first_one_b;
	int bitwidth;
    bitwidth=log(len)/log(2);
    first_one_a=(int*)malloc(sizeof(int)*len);
	first_one_b=(int*)malloc(sizeof(int)*len);
    
	for(i=0;i<len;i++)
	{
		first_one_a[i]=get_first_one(Sobol_seq[0][i],bitwidth);
        first_one_b[i]=get_first_one(Sobol_seq[1][i],bitwidth);
	}




	for(i = 0; i < out_neuron; i++)
	{
		bias[i]=quan_i_d(bias[i], integer,decimal, decimal_shift);
		for (j = 0; j < in_neuron; j++)
		{
			weights[i][j]=quan_i_d(weights[i][j], integer,decimal, decimal_shift);
		}
	}
	
	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			// output[i] += weights[i][j] * input[j];
			// output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);

			part_result = SC_Mul(weights[i][j], input[j],len,first_one_a,first_one_b);
			output[i]+=part_result;
			output[i] = quan_i_d(output[i], integer2,decimal2, decimal_shift2);
			//if(in_neuron==480 && i==0)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
		}
	}
	free(first_one_a);
	free(first_one_b);
	return 0;
}

int FC_ReLU // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output
)
{
	int i,j;
	//printf("weights[0][0]=%lf\n",weights[0][0]);
	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			output[i] += weights[i][j] * input[j];
			//if(in_neuron==480 && i==0)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
		}
		if(output[i]<0)
			output[i]=0;
	}
	
	return 0;
}



int FC_ReLU_quan // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2
)
{
	int i,j;
	//printf("weights[0][0]=%lf\n",weights[0][0]);
	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			output[i] += weights[i][j] * input[j];
			output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
			//if(in_neuron==480 && i==0)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
		}
		if(output[i]<0)
			output[i]=0;
	}
	
	return 0;
}

int FC_ReLU_quan_SC_Sobol_Clip // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2,
	double **Sobol_seq, //Sobol number[3][1023]
	int len,
	int scale_param
)
{
	int i,j;
	double part_result;
    
	int *first_one_a;
	int *first_one_b;
	int bitwidth;
    bitwidth=log(len)/log(2);
    first_one_a=(int*)malloc(sizeof(int)*len);
	first_one_b=(int*)malloc(sizeof(int)*len);

    for(i=0;i<len;i++)
	{
		first_one_a[i]=get_first_one(Sobol_seq[0][i],bitwidth);
        first_one_b[i]=get_first_one(Sobol_seq[1][i],bitwidth);
	}
	//printf("weights[0][0]=%lf\n",weights[0][0]);

	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		// output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			part_result = SC_Mul(weights[i][j], input[j]/scale_param,len,first_one_a,first_one_b);
			// part_result = SC_Mul_Comp(weights[i][j], input[j]/scale_param,len,Sobol_seq);
			output[i] += part_result*scale_param;
			// output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
			//if(in_neuron==480 && i==0)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
		}
		if(output[i]<0)
			output[i]=0;
		if(output[i]>1)
			output[i]=1;
	}
	free(first_one_a);
	free(first_one_b);
	return 0;
}

int FC_ReLU_quan_SC_Sobol // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2,
	double **Sobol_seq, //Sobol number[3][1023]
	int len,
	int scale_param
)
{
	int i,j;
	double part_result;
    
	int *first_one_a;
	int *first_one_b;
	int bitwidth;
    bitwidth=log(len)/log(2);
    first_one_a=(int*)malloc(sizeof(int)*len);
	first_one_b=(int*)malloc(sizeof(int)*len);

    for(i=0;i<len;i++)
	{
		first_one_a[i]=get_first_one(Sobol_seq[0][i],bitwidth);
        first_one_b[i]=get_first_one(Sobol_seq[1][i],bitwidth);
	}
	//printf("weights[0][0]=%lf\n",weights[0][0]);

	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			part_result = SC_Mul(weights[i][j], input[j]/scale_param,len,first_one_a,first_one_b);
			// part_result = SC_Mul_Comp(weights[i][j], input[j]/scale_param,len,Sobol_seq);
			output[i] += part_result*scale_param;
			output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
			//if(in_neuron==480 && i==0)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
		}
		if(output[i]<0)
			output[i]=0;
	}
	free(first_one_a);
	free(first_one_b);
	return 0;
}

//Assume that weights and biases have been quantized
int FC_quan2 // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2
)
{
	int i,j;
	
	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		//if(out_neuron==500 && i==9)
		//if(out_neuron==800 && i==43)
			//printf("bias=%lf\n",bias[i]);
		output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			output[i] += weights[i][j] * input[j];
			output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
			//if(out_neuron==500 && i==9 && j==43)
			//if(out_neuron==800 && i==43)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
			//if(out_neuron==500 && i==9)
				//printf("%d: %lf\n",j,output[i]);
		}
		//if(out_neuron==500 && i==9)
			//printf("output==%lf\n",output[i]);
	}
	
	return 0;
}

int FC_quan2_SC_Sobol_Clip // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2,
	double **Sobol_seq, //Sobol number[3][1023]
	int len,
	int scale_param
)
{
	int i,j;
	double part_result;
    
	int *first_one_a;
	int *first_one_b;
	int bitwidth;
    bitwidth=log(len)/log(2);
    first_one_a=(int*)malloc(sizeof(int)*len);
	first_one_b=(int*)malloc(sizeof(int)*len);

    for(i=0;i<len;i++)
	{
		first_one_a[i]=get_first_one(Sobol_seq[0][i],bitwidth);
        first_one_b[i]=get_first_one(Sobol_seq[1][i],bitwidth);
	}
	//printf("weights[0][0]=%lf\n",weights[0][0]);

	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		// output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			part_result = SC_Mul(weights[i][j], input[j]/scale_param,len,first_one_a,first_one_b);
			// part_result = SC_Mul_Comp(weights[i][j], input[j]/scale_param,len,Sobol_seq);
			output[i] += part_result*scale_param;
			// output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
			//if(in_neuron==480 && i==0)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
		}
	}
	free(first_one_a);
	free(first_one_b);
	return 0;
}

int FC_quan2_SC_Sobol // Fully connection
(
	int in_neuron, //The number of input neurons
	int out_neuron, // The number of output neurons
	double **weights, // Weight array with size out_neuron*in_neuron
	double *bias, // out_neuron biases
	double *input,
	double *output,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2,
	double **Sobol_seq, //Sobol number[3][1023]
	int len,
	int scale_param
)
{
	int i,j;
	double part_result;
    
	int *first_one_a;
	int *first_one_b;
	int bitwidth;
    bitwidth=log(len)/log(2);
    first_one_a=(int*)malloc(sizeof(int)*len);
	first_one_b=(int*)malloc(sizeof(int)*len);

    for(i=0;i<len;i++)
	{
		first_one_a[i]=get_first_one(Sobol_seq[0][i],bitwidth);
        first_one_b[i]=get_first_one(Sobol_seq[1][i],bitwidth);
	}
	//printf("weights[0][0]=%lf\n",weights[0][0]);

	for (i = 0; i < out_neuron; i++)
	{
		
		//printf("%d\n",i);
		output[i]=bias[i];
		// output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
		for (j = 0; j < in_neuron; j++)
		{
			//printf("%d\n",j);
			part_result = SC_Mul(weights[i][j], input[j]/scale_param,len,first_one_a,first_one_b);
			// part_result = SC_Mul_Comp(weights[i][j], input[j]/scale_param,len,Sobol_seq);
			output[i] += part_result*scale_param;
			// output[i]=quan_i_d(output[i], integer2,decimal2, decimal_shift2);
			//if(in_neuron==480 && i==0)
				//printf("%d: weight=%f,input=%f,output=%f\n",j,weights[i][j],input[j],output[i]);
		}
	}
	free(first_one_a);
	free(first_one_b);
	return 0;
}


int Conv // Convolutional layer
(
	int m, // Kernel size is m*n
	int n,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map: H_out=(H-(m-stride))/stride
	int W_out, // Width of output feature map: W_out=(W-(n-stride))/stride
	int num_in_channel, // Number of input channels
	int num_out_channel, // Number of output channels
	double ****weights, // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	double *bias, // num_out_channel biases
	int stride,
	double ***input, // num_in_channel*H*W
	double ***output // num_out_channel*H_out*W_out
)
{
	int i,j,k,ii,jj,kk;
	double max;
	max=0;
	
	//printf("Convolution!\n");
	for(i=0;i<num_out_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=bias[i];
				//if(i==1 && j==0 && k==5)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				for(ii=0;ii<num_in_channel;ii++)
				{
					for(jj=0;jj<m;jj++)
					{
						for(kk=0;kk<n;kk++)
						{
							output[i][j][k]+=weights[i][ii][jj][kk]*input[ii][j*stride+jj][k*stride+kk];
							//if(i==1 && j==0 && k==5)printf("weight=%lf, input=%lf, output[i][j][k]=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],output[i][j][k]);
						}
					}
				}
				// if(output[i][j][k] > max) max = output[i][j][k];
			} 
		}
	}
	// printf("max_output=%lf\n",max);
	
	return 0;
}




int Conv_SC_Sobol // Convolutional layer
(
	int m, // Kernel size is m*n
	int n,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map: H_out=(H-(m-stride))/stride
	int W_out, // Width of output feature map: W_out=(W-(n-stride))/stride
	int num_in_channel, // Number of input channels
	int num_out_channel, // Number of output channels
	double ****weights, // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	double *bias, // num_out_channel biases
	int stride,
	double ***input, // num_in_channel*H*W
	double ***output, // num_out_channel*H_out*W_out
	double **Sobol_seq, //Sobol number[3][1023]
	int len
)
{
	int i,j,k,ii,jj,kk;
	double part_result;
	double flip_result;

	int *first_one_a;
	int *first_one_b;
	int bitwidth;
    bitwidth=log(len)/log(2);
    first_one_a=(int*)malloc(sizeof(int)*len);
	first_one_b=(int*)malloc(sizeof(int)*len);
    
	for(i=0;i<len;i++)
	{
		first_one_a[i]=get_first_one(Sobol_seq[0][i],bitwidth);
        first_one_b[i]=get_first_one(Sobol_seq[1][i],bitwidth);
	}

	double max;
    max = 0;
	//printf("Convolution!\n");
	for(i=0;i<num_out_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=bias[i];
				//if(i==1 && j==0 && k==5)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				for(ii=0;ii<num_in_channel;ii++)
				{
					for(jj=0;jj<m;jj++)
					{
						for(kk=0;kk<n;kk++)
						{
							// output[i][j][k]+=weights[i][ii][jj][kk]*input[ii][j*stride+jj][k*stride+kk];
							part_result = SC_Mul(weights[i][ii][jj][kk], input[ii][j*stride+jj][k*stride+kk],len,first_one_a,first_one_b);
							// part_result = SC_Mul_Comp(weights[i][ii][jj][kk], input[ii][j*stride+jj][k*stride+kk],len,Sobol_seq);
							// flip_result = weights[i][ii][jj][kk] * input[ii][j*stride+jj][k*stride+kk];
							//if (flip_result!=0) printf("%d: part_result=%f,flip_result=%f\n",i, part_result, flip_result);
							output[i][j][k]+= part_result;
							//if(i==1 && j==0 && k==5)printf("weight=%lf, input=%lf, output[i][j][k]=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],output[i][j][k]);
						}
					}
				}
				// if(output[i][j][k] > max) max = output[i][j][k];
			} 
		}
	}
	// printf("max_output=%lf\n",max);
	
	return 0;
}






int Conv_quan_SC_Sobol // Convolutional layer
(
	int m, // Kernel size is m*n
	int n,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map: H_out=(H-(m-stride))/stride
	int W_out, // Width of output feature map: W_out=(W-(n-stride))/stride
	int num_in_channel, // Number of input channels
	int num_out_channel, // Number of output channels
	double ****weights, // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	double *bias, // num_out_channel biases
	int stride,
	double ***input, // num_in_channel*H*W
	double ***output, // num_out_channel*H_out*W_out
	int integer, 
	double decimal, 
	int decimal_shift,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2,
	double **Sobol_seq, //Sobol number[3][1023]
	int len
)
{
	int i,j,k,ii,jj,kk;
	double tmp;
	double part_result;
	int *first_one_a;
	int *first_one_b;
	int bitwidth;
    bitwidth=log(len)/log(2);
    first_one_a=(int*)malloc(sizeof(int)*len);
	first_one_b=(int*)malloc(sizeof(int)*len);
    
	for(i=0;i<len;i++)
	{
		first_one_a[i]=get_first_one(Sobol_seq[0][i],bitwidth);
        first_one_b[i]=get_first_one(Sobol_seq[1][i],bitwidth);
	}
	/*
	//quantiza the inputs
	for(i=0;i<num_in_channel;i++)
	{
		for(j=0;j<H_in;j++)
		{
			for(k=0;k<W_in;k++)
			{
				input[i][j][k]=
			}
		}
	}
	*/
	//printf("integer=%d, decimal=%lf, decimal_shift=%d\n",integer,decimal, decimal_shift);
	//quantize the weights and biases
	for(i=0;i<num_out_channel;i++)
	{
		//printf("bias[%d]=%lf\n",i,bias[i]);
		bias[i]=quan_i_d(bias[i], integer,decimal, decimal_shift);
		//bias[i]=quan_i_d(bias[i], 0,0.99609375, 256);
		//printf("bias[%d]=%lf\n",i,bias[i]);
		for(j=0;j<num_in_channel;j++)
		{
			for(ii=0;ii<m;ii++)
			{
				for(jj=0;jj<n;jj++)
				{
					weights[i][j][ii][jj] = quan_i_d(weights[i][j][ii][jj], integer,decimal, decimal_shift);
				}
			}
		}
	}
	
	
	//printf("Convolution!\n");
	for(i=0;i<num_out_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=bias[i];
				output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
				//output[i][j][k]=quan_i_d(bias[i], integer,decimal, decimal_shift);
				//if(i==1 && j==0 && k==5)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				for(ii=0;ii<num_in_channel;ii++)
				{
					for(jj=0;jj<m;jj++)
					{
						for(kk=0;kk<n;kk++)
						{
							part_result = SC_Mul(weights[i][ii][jj][kk], input[ii][j*stride+jj][k*stride+kk],len,first_one_a,first_one_b);
							// part_result = SC_Mul_Comp(weights[i][ii][jj][kk], input[ii][j*stride+jj][k*stride+kk],len,Sobol_seq);
							output[i][j][k]+=part_result;
							//if(num_in_channel==20)printf("%lf ",output[i][j][k]);
							//if(num_in_channel==1)
							output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
							// if(i==1 && j==0 && k==5)printf("weight=%lf, input=%lf, output[i][j][k]=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],output[i][j][k]);
						}
					}
				}
			}
		}
	}
	free(first_one_a);
	free(first_one_b);
	return 0;
}

int Conv_quan // Convolutional layer
(
	int m, // Kernel size is m*n
	int n,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map: H_out=(H-(m-stride))/stride
	int W_out, // Width of output feature map: W_out=(W-(n-stride))/stride
	int num_in_channel, // Number of input channels
	int num_out_channel, // Number of output channels
	double ****weights, // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	double *bias, // num_out_channel biases
	int stride,
	double ***input, // num_in_channel*H*W
	double ***output, // num_out_channel*H_out*W_out
	int integer, 
	double decimal, 
	int decimal_shift,
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2
)
{
	int i,j,k,ii,jj,kk;
	double tmp;
	/*
	//quantiza the inputs
	for(i=0;i<num_in_channel;i++)
	{
		for(j=0;j<H_in;j++)
		{
			for(k=0;k<W_in;k++)
			{
				input[i][j][k]=
			}
		}
	}
	*/
	//printf("integer=%d, decimal=%lf, decimal_shift=%d\n",integer,decimal, decimal_shift);
	//quantize the weights and biases
	for(i=0;i<num_out_channel;i++)
	{
		//printf("bias[%d]=%lf\n",i,bias[i]);
		bias[i]=quan_i_d(bias[i], integer,decimal, decimal_shift);
		//bias[i]=quan_i_d(bias[i], 0,0.99609375, 256);
		//printf("bias[%d]=%lf\n",i,bias[i]);
		for(j=0;j<num_in_channel;j++)
		{
			for(ii=0;ii<m;ii++)
			{
				for(jj=0;jj<n;jj++)
				{
					weights[i][j][ii][jj] = quan_i_d(weights[i][j][ii][jj], integer,decimal, decimal_shift);
				}
			}
		}
	}
	
	
	//printf("Convolution!\n");
	for(i=0;i<num_out_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=bias[i];
				output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
				//output[i][j][k]=quan_i_d(bias[i], integer,decimal, decimal_shift);
				//if(i==1 && j==0 && k==5)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				for(ii=0;ii<num_in_channel;ii++)
				{
					for(jj=0;jj<m;jj++)
					{
						for(kk=0;kk<n;kk++)
						{
							output[i][j][k]+=weights[i][ii][jj][kk]*input[ii][j*stride+jj][k*stride+kk];
							//if(num_in_channel==20)printf("%lf ",output[i][j][k]);
							//if(num_in_channel==1)
								output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
							//if(i==1 && j==0 && k==5)printf("weight=%lf, input=%lf, output[i][j][k]=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],output[i][j][k]);
						}
					}
				}
			}
		}
	}
	
	return 0;
}


int Conv_ReLU // Convolutional layer
(
	int m, // Kernel size is m*n
	int n,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map: H_out=(H-(m-stride))/stride
	int W_out, // Width of output feature map: W_out=(W-(n-stride))/stride
	int num_in_channel, // Number of input channels
	int num_out_channel, // Number of output channels
	double ****weights, // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	double *bias, // num_out_channel biases
	int stride,
	double ***input, // num_in_channel*H*W
	double ***output // num_out_channel*H_out*W_out
)
{
	int i,j,k,ii,jj,kk;
	
	//printf("Convolution!\n");
	for(i=0;i<num_out_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=bias[i];
				//if(i==0 && j==0 && k==0 && num_in_channel==3)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				for(ii=0;ii<num_in_channel;ii++)
				{
					for(jj=0;jj<m;jj++)
					{
						for(kk=0;kk<n;kk++)
						{
							output[i][j][k]+=weights[i][ii][jj][kk]*input[ii][j*stride+jj][k*stride+kk];
							//if(i==1 && j==0 && k==5)printf("weight=%lf, input=%lf, output[i][j][k]=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],output[i][j][k]);
						}
					}
				}
				//if(i==0 && j==0 && k==0 && num_in_channel==3)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				if(output[i][j][k]<0)
					output[i][j][k]=0;
			}
		}
	}
	
	return 0;
}

int Conv_ReLU_quan // Convolutional layer
(
	int m, // Kernel size is m*n
	int n,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map: H_out=(H-(m-stride))/stride
	int W_out, // Width of output feature map: W_out=(W-(n-stride))/stride
	int num_in_channel, // Number of input channels
	int num_out_channel, // Number of output channels
	double ****weights, // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	double *bias, // num_out_channel biases
	int stride,
	double ***input, // num_in_channel*H*W
	double ***output, // num_out_channel*H_out*W_out
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2
)
{
	int i,j,k,ii,jj,kk;
	
	//printf("Convolution!\n");
	for(i=0;i<num_out_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=bias[i];
				output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
				//if(i==0 && j==0 && k==0 && num_in_channel==3)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				for(ii=0;ii<num_in_channel;ii++)
				{
					for(jj=0;jj<m;jj++)
					{
						for(kk=0;kk<n;kk++)
						{
							output[i][j][k]+=weights[i][ii][jj][kk]*input[ii][j*stride+jj][k*stride+kk];
							output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
							//if(i==1 && j==0 && k==5)printf("weight=%lf, input=%lf, output[i][j][k]=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],output[i][j][k]);
						}
					}
				}
				//if(i==0 && j==0 && k==0 && num_in_channel==3)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				if(output[i][j][k]<0)
					output[i][j][k]=0;
			}
		}
	}
	
	return 0;
}

int Conv_ReLU_quan_SC_Sobol // Convolutional layer
(
	int m, // Kernel size is m*n
	int n,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map: H_out=(H-(m-stride))/stride
	int W_out, // Width of output feature map: W_out=(W-(n-stride))/stride
	int num_in_channel, // Number of input channels
	int num_out_channel, // Number of output channels
	double ****weights, // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	double *bias, // num_out_channel biases
	int stride,
	double ***input, // num_in_channel*H*W
	double ***output, // num_out_channel*H_out*W_out
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2,
	double **Sobol_seq, //Sobol number[3][1023]
	int len,
	int scale_param
)
{
	int i,j,k,ii,jj,kk;
	double tmp;
	double part_result;
	double flip_result;
	int *first_one_a;
	int *first_one_b;
	int bitwidth;
	double max_input=0;
	double max_weight=0;
    bitwidth=log(len)/log(2);
    first_one_a=(int*)malloc(sizeof(int)*len);
	first_one_b=(int*)malloc(sizeof(int)*len);
    
	for(i=0;i<len;i++)
	{
		first_one_a[i]=get_first_one(Sobol_seq[0][i],bitwidth);
        first_one_b[i]=get_first_one(Sobol_seq[1][i],bitwidth);
	}
	//printf("Convolution!\n");
	for(i=0;i<num_out_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=bias[i];
				output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
				//if(i==0 && j==0 && k==0 && num_in_channel==3)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				for(ii=0;ii<num_in_channel;ii++)
				{
					for(jj=0;jj<m;jj++)
					{
						for(kk=0;kk<n;kk++)
						{
							part_result = SC_Mul(weights[i][ii][jj][kk]/scale_param, input[ii][j*stride+jj][k*stride+kk],len,first_one_a,first_one_b);
							// part_result = SC_Mul_Comp(weights[i][ii][jj][kk]/scale_param, input[ii][j*stride+jj][k*stride+kk],len,Sobol_seq);
							flip_result = weights[i][ii][jj][kk]*input[ii][j*stride+jj][k*stride+kk]/scale_param;
							output[i][j][k]+=part_result*scale_param;
							// output[i][j][k]+=flip_result*scale_param;
							output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
							//if(i==1 && j==0 && k==5)printf("weight=%lf, input=%lf, output[i][j][k]=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],output[i][j][k]);
							// if(flip_result != 0)printf("weight=%lf, input=%lf, flip_result=%lf, part_result=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],flip_result/scale_param,part_result);
							// if(input[ii][j*stride+jj][k*stride+kk] > max_input) max_input = input[ii][j*stride+jj][k*stride+kk];
							// if(weights[i][ii][jj][kk] > max_weight) max_weight = weights[i][ii][jj][kk];
						}
					}
				}
				//if(i==0 && j==0 && k==0 && num_in_channel==3)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				if(output[i][j][k]<0)
					output[i][j][k]=0;
			}
		}
	}
	// printf("max_input=%lf,max_weight=%lf\n",max_input,max_weight);
	free(first_one_a);
	free(first_one_b);
	return 0;
}

int Conv_ReLU_quan_SC_Sobol_Clip // Convolutional layer
(
	int m, // Kernel size is m*n
	int n,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map: H_out=(H-(m-stride))/stride
	int W_out, // Width of output feature map: W_out=(W-(n-stride))/stride
	int num_in_channel, // Number of input channels
	int num_out_channel, // Number of output channels
	double ****weights, // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	double *bias, // num_out_channel biases
	int stride,
	double ***input, // num_in_channel*H*W
	double ***output, // num_out_channel*H_out*W_out
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2,
	double **Sobol_seq, //Sobol number[3][1023]
	int len,
	int scale_param
)
{
	int i,j,k,ii,jj,kk;
	double tmp;
	double part_result;
	double flip_result;
	int *first_one_a;
	int *first_one_b;
	int bitwidth;
    bitwidth=log(len)/log(2);
    first_one_a=(int*)malloc(sizeof(int)*len);
	first_one_b=(int*)malloc(sizeof(int)*len);
    
	for(i=0;i<len;i++)
	{
		first_one_a[i]=get_first_one(Sobol_seq[0][i],bitwidth);
        first_one_b[i]=get_first_one(Sobol_seq[1][i],bitwidth);
	}
	//printf("Convolution!\n");
	for(i=0;i<num_out_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=bias[i];
				// output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
				//if(i==0 && j==0 && k==0 && num_in_channel==3)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				for(ii=0;ii<num_in_channel;ii++)
				{
					for(jj=0;jj<m;jj++)
					{
						for(kk=0;kk<n;kk++)
						{
							part_result = SC_Mul(weights[i][ii][jj][kk], input[ii][j*stride+jj][k*stride+kk]/scale_param,len,first_one_a,first_one_b);
							// part_result = SC_Mul_Comp(weights[i][ii][jj][kk], input[ii][j*stride+jj][k*stride+kk]/scale_param,len,Sobol_seq);
							flip_result = weights[i][ii][jj][kk]*input[ii][j*stride+jj][k*stride+kk];
							output[i][j][k]+=part_result*scale_param;
							// output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
							//if(i==1 && j==0 && k==5)printf("weight=%lf, input=%lf, output[i][j][k]=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],output[i][j][k]);
							// if(flip_result != 0)printf("weight=%lf, input=%lf, flip_result=%lf, part_result=%lf\n",weights[i][ii][jj][kk],input[ii][j*stride+jj][k*stride+kk],flip_result/scale_param,part_result);
						}
					}
				}
				//if(i==0 && j==0 && k==0 && num_in_channel==3)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				if(output[i][j][k]<0)
					output[i][j][k]=0;
				if(output[i][j][k]>1)
					output[i][j][k]=1;
				// if(output[i][j][k] != 0 )printf("output[%d][%d][%d]=%lf \n",i,j,k,output[i][j][k]);
				// printf("output[%d][%d][%d]=%lf \n",i,j,k,output[i][j][k]);
			}
		}
	}
	free(first_one_a);
	free(first_one_b);
	return 0;
}



// Assume that weishts and biases have been quantized
int Conv_quan2 // Convolutional layer
(
	int m, // Kernel size is m*n
	int n,
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int H_out, // Height of output feature map: H_out=(H-(m-stride))/stride
	int W_out, // Width of output feature map: W_out=(W-(n-stride))/stride
	int num_in_channel, // Number of input channels
	int num_out_channel, // Number of output channels
	double ****weights, // The are num_out_channel filters, each of which contains num_in_channel kernels. The size of weight is num_out_channel*num_in_channel_m*n
	double *bias, // num_out_channel biases
	int stride,
	double ***input, // num_in_channel*H*W
	double ***output, // num_out_channel*H_out*W_out
	int integer2, // For accumulation
	double decimal2, 
	int decimal_shift2
)
{
	int i,j,k,ii,jj,kk;
	double tmp;
	/*
	//quantiza the inputs
	for(i=0;i<num_in_channel;i++)
	{
		for(j=0;j<H_in;j++)
		{
			for(k=0;k<W_in;k++)
			{
				input[i][j][k]=
			}
		}
	}
	*/

	
	//printf("Convolution!\n");
	for(i=0;i<num_out_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=bias[i];
				output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
				//output[i][j][k]=quan_i_d(bias[i], integer,decimal, decimal_shift);
				//if(i==1 && j==0 && k==5)printf("output[i][j][k]=%lf\n",output[i][j][k]);
				for(ii=0;ii<num_in_channel;ii++)
				{
					for(jj=0;jj<m;jj++)
					{
						for(kk=0;kk<n;kk++)
						{
							output[i][j][k]+=weights[i][ii][jj][kk]*input[ii][j*stride+jj][k*stride+kk];
							//if(num_in_channel==20)printf("%lf ",output[i][j][k]);
							//if(num_in_channel==1)
								output[i][j][k] = quan_i_d(output[i][j][k], integer2,decimal2, decimal_shift2);
							//if(H_in==28 && i==0 && j==16 && k==14)
								//printf("weight=%lf, input[%d][%d][%d]=%lf, output[i][j][k]=%lf\n",weights[i][ii][jj][kk],ii,j*stride+jj,k*stride+kk,input[ii][j*stride+jj][k*stride+kk],output[i][j][k]);
						}
					}
				}
			}
		}
	}
	
	return 0;
}


int max_pooling
(
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int strideH,
	int strideW,
	int H_out, // Height of output feature map: H_out=H/strideH
	int W_out, // Width of output feature map: W_out=W/strideW
	int num_channel, // Number of channels
	double ***input, // num_channel*H*W
	double ***output // num_channel*H_out*W_out
)
{
	int i,j,k,ii,jj,kk;
	double max;
	
	for(i=0;i<num_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				output[i][j][k]=-999999999;
				max=output[i][j][k];
				for(jj=j*strideH;jj<(j+1)*strideH;jj++)
				{
					for(kk=k*strideW;kk<(k+1)*strideW;kk++)
					{
						//if(i==0 && j==0 && k==0)printf("jj=%d,kk=%d,input[i][jj][kk]=%lf,max=%lf\n",jj,kk,input[i][jj][kk],max);
						if(input[i][jj][kk]>max)
							max=input[i][jj][kk];
					}
				}
				output[i][j][k]=max;
			}
			
		}
		
	}
}


int average_pooling
(
	int H_in, // Height of input feature map
	int W_in, // Width of input feature map
	int strideH,
	int strideW,
	int H_out, // Height of output feature map: H_out=H/strideH
	int W_out, // Width of output feature map: W_out=W/strideW
	int num_channel, // Number of channels
	double ***input, // num_channel*H*W
	double ***output // num_channel*H_out*W_out
)
{
	int i,j,k,ii,jj,kk;
	double sum;
	
	for(i=0;i<num_channel;i++)
	{
		for(j=0;j<H_out;j++)
		{
			for(k=0;k<W_out;k++)
			{
				//output[i][j][k]=-999999999;
				sum=0;
				for(jj=j*strideH;jj<(j+1)*strideH;jj++)
				{
					for(kk=k*strideW;kk<(k+1)*strideW;kk++)
					{
						//if(H_in==8 && i==0 && j==3 && k==3)
						//if(H_in==24 && i==0 && j==8 && k==7)
							//printf("jj=%d,kk=%d,input[i][jj][kk]=%lf,max=%lf\n",jj,kk,input[i][jj][kk],max);
						//if(input[i][jj][kk]>max)
						//	max=input[i][jj][kk];
						sum+=input[i][jj][kk];
					}
				}
				output[i][j][k]=sum/(strideH*strideW);
				//if(H_in==8 && i==0 && j==3 && k==3)printf("output[%d][%d][%d]=%lf\n",i,j,k,output[i][j][k]);
			}
			
		}
		
	}
}



int ReLU(int number, double * input, double * output)
{
	int i;
	for(i=0;i<number;i++)
		output[i]=(input[i]<0)?0:input[i];
	
	return 0;
}

